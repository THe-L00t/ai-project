#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ê¸°ë°˜ ì½”ì¸ ì˜ˆì¸¡ AI
- ì „ ì„¸ê³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°ì • ë¶„ì„
- ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§
- ê°ì • ì ìˆ˜ë¥¼ í†µí•œ ê°€ê²© ë³€ë™ ì˜ˆì¸¡
- 1ì‹œê°„ ë‚´ í•™ìŠµ ì™„ë£Œ ìµœì í™”
"""

import os
import sys
import time
import logging
import asyncio
import aiohttp
import feedparser
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque
import pickle
import json

# NLP ë¼ì´ë¸ŒëŸ¬ë¦¬
from textblob import TextBlob
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import nltk

# ë¨¸ì‹ ëŸ¬ë‹
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from exchange.UpbitAPI import UpbitAPI

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/news_sentiment_ai.log')
    ]
)
logger = logging.getLogger(__name__)

class NewsCollector:
    """ì „ ì„¸ê³„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        # í¬ë¦½í†  ê´€ë ¨ RSS í”¼ë“œë“¤
        self.crypto_feeds = [
            'https://cointelegraph.com/rss',
            'https://cryptonews.com/news/feed/',
            'https://www.coindesk.com/arc/outboundfeeds/rss/',
            'https://decrypt.co/feed',
            'https://bitcoinmagazine.com/.rss/full/',
            'https://cryptobriefing.com/feed/',
            'https://www.crypto-news.net/feed/',
            'https://ambcrypto.com/feed/',
            'https://cryptopotato.com/feed/',
            'https://u.today/rss'
        ]

        # ì¼ë°˜ ê²½ì œ/ê¸°ìˆ  ë‰´ìŠ¤
        self.general_feeds = [
            'https://feeds.reuters.com/reuters/businessNews',
            'https://rss.cnn.com/rss/money_news_economy.rss',
            'https://feeds.bloomberg.com/markets/news.rss',
            'https://rss.cbc.ca/lineup/business.xml',
            'https://www.cnbc.com/id/100003114/device/rss/rss.html'
        ]

        # ì½”ì¸ë³„ í‚¤ì›Œë“œ
        self.coin_keywords = {
            'BTC': ['bitcoin', 'btc', 'satoshi', 'digital gold'],
            'ETH': ['ethereum', 'eth', 'vitalik', 'smart contract', 'defi'],
            'ADA': ['cardano', 'ada', 'charles hoskinson', 'proof of stake'],
            'DOT': ['polkadot', 'dot', 'parachain', 'substrate']
        }

    async def fetch_feed(self, session, url):
        """RSS í”¼ë“œ ë¹„ë™ê¸° ìˆ˜ì§‘ (SSL ë¬¸ì œ í•´ê²°)"""
        try:
            # SSL ê²€ì¦ ë¹„í™œì„±í™”
            connector = aiohttp.TCPConnector(ssl=False)
            async with aiohttp.ClientSession(connector=connector) as ssl_session:
                async with ssl_session.get(url, timeout=10) as response:
                    if response.status == 200:
                        content = await response.text()
                        return feedparser.parse(content)
        except Exception as e:
            logger.error(f"í”¼ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            # ë°±ì—…: requests ì‚¬ìš©
            try:
                import requests
                requests.packages.urllib3.disable_warnings()
                response = requests.get(url, timeout=10, verify=False)
                if response.status_code == 200:
                    return feedparser.parse(response.content)
            except:
                pass
            return None

    async def collect_news_async(self, max_articles=200):
        """ë¹„ë™ê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœì í™”)"""
        logger.info("ğŸŒ ì „ ì„¸ê³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        start_time = time.time()

        all_feeds = self.crypto_feeds + self.general_feeds
        articles = []

        # SSL ê²€ì¦ ë¹„í™œì„±í™”
        connector = aiohttp.TCPConnector(ssl=False)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [self.fetch_feed(session, url) for url in all_feeds]

            for i, task in enumerate(asyncio.as_completed(tasks)):
                try:
                    feed = await task
                    if feed and hasattr(feed, 'entries'):
                        for entry in feed.entries[:20]:  # í”¼ë“œë‹¹ ìµœëŒ€ 20ê°œ
                            if len(articles) >= max_articles:
                                break

                            article = {
                                'title': entry.get('title', ''),
                                'description': entry.get('description', ''),
                                'link': entry.get('link', ''),
                                'published': entry.get('published', ''),
                                'source': all_feeds[i % len(all_feeds)],
                                'timestamp': datetime.now()
                            }
                            articles.append(article)

                        logger.info(f"ğŸ“° í”¼ë“œ {i+1}/{len(all_feeds)} ì²˜ë¦¬ ì™„ë£Œ ({len(feed.entries)}ê°œ ê¸°ì‚¬)")

                    if len(articles) >= max_articles:
                        break

                except Exception as e:
                    logger.error(f"í”¼ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        elapsed = time.time() - start_time
        logger.info(f"âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ({elapsed:.1f}ì´ˆ)")

        return articles

    def filter_crypto_news(self, articles):
        """ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§"""
        filtered_news = {coin: [] for coin in self.coin_keywords.keys()}

        for article in articles:
            text = (article['title'] + ' ' + article['description']).lower()

            for coin, keywords in self.coin_keywords.items():
                for keyword in keywords:
                    if keyword in text:
                        filtered_news[coin].append(article)
                        break

        logger.info("ğŸ” ì½”ì¸ë³„ ë‰´ìŠ¤ í•„í„°ë§ ì™„ë£Œ:")
        for coin, news_list in filtered_news.items():
            logger.info(f"   {coin}: {len(news_list)}ê°œ ê¸°ì‚¬")

        return filtered_news

class SentimentAnalyzer:
    """ê°ì • ë¶„ì„ ì—”ì§„"""

    def __init__(self):
        # ê²½ëŸ‰ ê°ì • ë¶„ì„ ëª¨ë¸ ì‚¬ìš© (1ì‹œê°„ ì œí•œì„ ìœ„í•´)
        try:
            # FinBERT ëŒ€ì‹  ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                tokenizer="cardiffnlp/twitter-roberta-base-sentiment-latest",
                truncation=True,
                max_length=512
            )
            logger.info("ğŸ§  ê³ ê¸‰ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, TextBlob ì‚¬ìš©: {e}")
            self.sentiment_pipeline = None

        # í¬ë¦½í†  íŠ¹í™” í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        self.crypto_weights = {
            'positive': ['bullish', 'moon', 'pump', 'rally', 'surge', 'adoption', 'breakthrough'],
            'negative': ['bearish', 'crash', 'dump', 'ban', 'hack', 'regulation', 'selloff'],
            'neutral': ['stable', 'sideways', 'consolidation']
        }

    def analyze_sentiment_fast(self, text):
        """ë¹ ë¥¸ ê°ì • ë¶„ì„ (1ì‹œê°„ ì œí•œ ê³ ë ¤)"""
        if not text or len(text.strip()) < 10:
            return {'score': 0.0, 'confidence': 0.0}

        try:
            if self.sentiment_pipeline:
                # Transformer ëª¨ë¸ ì‚¬ìš© (ë” ì •í™•í•¨)
                result = self.sentiment_pipeline(text[:500])  # ê¸¸ì´ ì œí•œ

                label = result[0]['label'].upper()
                confidence = result[0]['score']

                if 'POSITIVE' in label:
                    score = confidence
                elif 'NEGATIVE' in label:
                    score = -confidence
                else:
                    score = 0.0

            else:
                # TextBlob ëŒ€ì•ˆ ì‚¬ìš©
                blob = TextBlob(text)
                score = blob.sentiment.polarity
                confidence = abs(blob.sentiment.subjectivity)

            # í¬ë¦½í†  íŠ¹í™” í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©
            text_lower = text.lower()
            for pos_word in self.crypto_weights['positive']:
                if pos_word in text_lower:
                    score += 0.1

            for neg_word in self.crypto_weights['negative']:
                if neg_word in text_lower:
                    score -= 0.1

            # ì ìˆ˜ ì •ê·œí™”
            score = max(-1.0, min(1.0, score))

            return {
                'score': float(score),
                'confidence': float(confidence),
                'method': 'transformer' if self.sentiment_pipeline else 'textblob'
            }

        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'score': 0.0, 'confidence': 0.0}

    def batch_analyze(self, articles, max_workers=4):
        """ë°°ì¹˜ ê°ì • ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìµœì í™”)"""
        logger.info(f"ğŸ­ {len(articles)}ê°œ ê¸°ì‚¬ ê°ì • ë¶„ì„ ì‹œì‘...")

        start_time = time.time()
        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # í…ìŠ¤íŠ¸ ì¤€ë¹„
            texts = []
            for article in articles:
                text = article['title'] + ' ' + article['description']
                texts.append(text[:1000])  # ê¸¸ì´ ì œí•œ

            # ë³‘ë ¬ ì²˜ë¦¬
            future_to_idx = {
                executor.submit(self.analyze_sentiment_fast, text): i
                for i, text in enumerate(texts)
            }

            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    sentiment = future.result()
                    results.append({
                        'article': articles[idx],
                        'sentiment': sentiment
                    })
                except Exception as e:
                    logger.error(f"ê¸°ì‚¬ {idx} ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                    results.append({
                        'article': articles[idx],
                        'sentiment': {'score': 0.0, 'confidence': 0.0}
                    })

        elapsed = time.time() - start_time
        logger.info(f"âœ… ê°ì • ë¶„ì„ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

        return results

class NewsSentimentAI:
    """ë‰´ìŠ¤ ê¸°ë°˜ ì½”ì¸ ì˜ˆì¸¡ AI (1ì‹œê°„ í•™ìŠµ ìµœì í™”)"""

    def __init__(self):
        load_dotenv()

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.upbit = UpbitAPI()
        self.news_collector = NewsCollector()
        self.sentiment_analyzer = SentimentAnalyzer()

        # ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ ì¶”ê°€
        try:
            from adaptive_learning_ai import AdaptiveLearningEngine
            self.adaptive_learner = AdaptiveLearningEngine()
            self.adaptive_learner.load_learning_state()
            self.adaptive_learner.initialize_online_models()
            self.adaptive_learning_enabled = True
            logger.info("ğŸ§  ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ ì—°ë™ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ ì—°ë™ ì‹¤íŒ¨: {e}")
            self.adaptive_learner = None
            self.adaptive_learning_enabled = False

        # ë°ì´í„° ì €ì¥ì†Œ
        self.news_history = deque(maxlen=1000)
        self.sentiment_history = {}
        self.price_correlation = {}

        # ëª¨ë¸
        self.prediction_models = {}
        self.scalers = {}

        # 1ì‹œê°„ í•™ìŠµ ìµœì í™” ì„¤ì •
        self.max_learning_time = 3600  # 1ì‹œê°„
        self.quick_mode = True  # ë¹ ë¥¸ í•™ìŠµ ëª¨ë“œ

        logger.info("ğŸŒ ë‰´ìŠ¤ ê¸°ë°˜ ì½”ì¸ ì˜ˆì¸¡ AI ì´ˆê¸°í™” ì™„ë£Œ")

    def collect_and_analyze_news(self):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„"""
        start_time = time.time()

        # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ (ë¹„ë™ê¸°, ìµœëŒ€ 5ë¶„)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        articles = loop.run_until_complete(
            self.news_collector.collect_news_async(max_articles=150)
        )
        loop.close()

        # 2. ì½”ì¸ë³„ í•„í„°ë§
        filtered_news = self.news_collector.filter_crypto_news(articles)

        # 3. ê°ì • ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬, ìµœëŒ€ 10ë¶„)
        coin_sentiments = {}
        for coin, news_list in filtered_news.items():
            if news_list:
                sentiment_results = self.sentiment_analyzer.batch_analyze(news_list)

                # í‰ê·  ê°ì • ì ìˆ˜ ê³„ì‚°
                scores = [r['sentiment']['score'] for r in sentiment_results]
                confidences = [r['sentiment']['confidence'] for r in sentiment_results]

                coin_sentiments[coin] = {
                    'avg_sentiment': np.mean(scores) if scores else 0.0,
                    'sentiment_strength': np.std(scores) if len(scores) > 1 else 0.0,
                    'avg_confidence': np.mean(confidences) if confidences else 0.0,
                    'news_count': len(news_list),
                    'timestamp': datetime.now()
                }

        # 4. íˆìŠ¤í† ë¦¬ì— ì €ì¥
        news_data = {
            'timestamp': datetime.now(),
            'articles_total': len(articles),
            'coin_sentiments': coin_sentiments
        }
        self.news_history.append(news_data)

        elapsed = time.time() - start_time
        logger.info(f"ğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

        return coin_sentiments

    def create_prediction_features(self, coin, hours_back=24):
        """ì˜ˆì¸¡ìš© íŠ¹ì„± ìƒì„± (ìµœì í™”)"""
        features = []

        # 1. ë‰´ìŠ¤ ê°ì • íŠ¹ì„± (ìµœê·¼ 24ì‹œê°„)
        recent_news = [n for n in self.news_history if
                      (datetime.now() - n['timestamp']).total_seconds() < hours_back * 3600]

        if recent_news and coin in recent_news[-1]['coin_sentiments']:
            recent_sentiments = []
            news_counts = []

            for news_data in recent_news:
                if coin in news_data['coin_sentiments']:
                    sentiment_info = news_data['coin_sentiments'][coin]
                    recent_sentiments.append(sentiment_info['avg_sentiment'])
                    news_counts.append(sentiment_info['news_count'])

            if recent_sentiments:
                features.extend([
                    np.mean(recent_sentiments),  # í‰ê·  ê°ì •
                    np.std(recent_sentiments) if len(recent_sentiments) > 1 else 0,  # ê°ì • ë³€ë™ì„±
                    max(recent_sentiments),  # ìµœê³  ê°ì •
                    min(recent_sentiments),  # ìµœì € ê°ì •
                    np.sum(news_counts),  # ì´ ë‰´ìŠ¤ ìˆ˜
                    len(recent_sentiments),  # ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜
                ])
            else:
                features.extend([0.0] * 6)
        else:
            features.extend([0.0] * 6)

        # 2. ê°€ê²© ê¸°ìˆ  ì§€í‘œ (ê°„ì†Œí™”)
        try:
            ticker = self.upbit.GetTicker([f'KRW-{coin}'])
            if ticker:
                price_data = ticker[0]
                features.extend([
                    float(price_data.change_rate),  # ë³€ë™ë¥ 
                    float(price_data.acc_trade_volume_24h),  # ê±°ë˜ëŸ‰
                    float(price_data.trade_price)  # í˜„ì¬ ê°€ê²© (ë¡œê·¸)
                ])
            else:
                features.extend([0.0] * 3)
        except:
            features.extend([0.0] * 3)

        return np.array(features)

    def train_quick_models(self, coins=['BTC', 'ETH', 'ADA', 'DOT']):
        """ë¹ ë¥¸ ëª¨ë¸ í›ˆë ¨ (1ì‹œê°„ ì œí•œ)"""
        logger.info("ğŸš€ ë¹ ë¥¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        train_start = time.time()

        for coin in coins:
            try:
                # ì‹œê°„ ì²´í¬
                elapsed = time.time() - train_start
                if elapsed > self.max_learning_time * 0.8:  # 80% ì‹œê°„ ì´ˆê³¼ì‹œ ì¤‘ë‹¨
                    logger.warning(f"â° ì‹œê°„ ì œí•œìœ¼ë¡œ {coin} ëª¨ë¸ í›ˆë ¨ ê±´ë„ˆëœ€")
                    continue

                logger.info(f"ğŸ“ {coin} ëª¨ë¸ í›ˆë ¨ ì¤‘...")

                # íŠ¹ì„± ë°ì´í„° ìƒì„± (ìµœê·¼ ë°ì´í„°ë§Œ)
                if len(self.news_history) < 10:
                    logger.warning(f"{coin}: í›ˆë ¨ ë°ì´í„° ë¶€ì¡±")
                    continue

                X = []
                y = []

                # ë¹ ë¥¸ ë°ì´í„° ì¤€ë¹„ (ìµœê·¼ 50ê°œë§Œ)
                for i in range(min(50, len(self.news_history) - 1)):
                    features = self.create_prediction_features(coin)

                    if len(features) > 0:
                        # ë‹¤ìŒ ì‹œê°„ì˜ ê°€ê²© ë³€í™”ë¥¼ ì˜ˆì¸¡
                        try:
                            current_ticker = self.upbit.GetTicker([f'KRW-{coin}'])
                            if current_ticker:
                                price_change = float(current_ticker[0].change_rate) * 100
                                X.append(features)
                                y.append(price_change)
                        except:
                            continue

                if len(X) < 5:
                    logger.warning(f"{coin}: ì¶©ë¶„í•œ í›ˆë ¨ ë°ì´í„° ì—†ìŒ")
                    continue

                X = np.array(X)
                y = np.array(y)

                # ë¹ ë¥¸ ëª¨ë¸ (GradientBoosting ëŒ€ì‹  RandomForest)
                model = RandomForestRegressor(
                    n_estimators=50,  # íŠ¸ë¦¬ ìˆ˜ ì¤„ì„
                    max_depth=10,
                    random_state=42,
                    n_jobs=-1  # ë³‘ë ¬ ì²˜ë¦¬
                )

                # ë°ì´í„° ì •ê·œí™”
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                # ëª¨ë¸ í›ˆë ¨
                model.fit(X_scaled, y)

                # ì„±ëŠ¥ í‰ê°€ (ê°„ì†Œí™”)
                y_pred = model.predict(X_scaled)
                mae = mean_absolute_error(y, y_pred)

                # ì €ì¥
                self.prediction_models[coin] = model
                self.scalers[coin] = scaler

                logger.info(f"âœ… {coin} ëª¨ë¸ ì™„ë£Œ (MAE: {mae:.2f}%)")

            except Exception as e:
                logger.error(f"{coin} ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")

        total_elapsed = time.time() - train_start
        logger.info(f"ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ ({total_elapsed:.1f}ì´ˆ)")

    def predict_price_movement(self, coin, horizon_hours=1):
        """ê°€ê²© ë³€ë™ ì˜ˆì¸¡ (ì ì‘í˜• í•™ìŠµ í¬í•¨)"""
        predictions = []

        # 1. ê¸°ì¡´ ë‰´ìŠ¤ ê¸°ë°˜ ì˜ˆì¸¡
        if coin in self.prediction_models:
            try:
                features = self.create_prediction_features(coin)
                if len(features) > 0:
                    features_scaled = self.scalers[coin].transform(features.reshape(1, -1))
                    predicted_change = self.prediction_models[coin].predict(features_scaled)[0]
                    predictions.append({
                        'source': 'news_model',
                        'predicted_change': predicted_change,
                        'confidence': 0.6
                    })
            except Exception as e:
                logger.error(f"{coin} ë‰´ìŠ¤ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

        # 2. ì ì‘í˜• í•™ìŠµ ì˜ˆì¸¡
        if self.adaptive_learning_enabled and self.adaptive_learner:
            try:
                adaptive_prediction = self.adaptive_learner.get_enhanced_prediction(coin)
                if adaptive_prediction:
                    predictions.append({
                        'source': 'adaptive_model',
                        'predicted_change': adaptive_prediction['predicted_change_pct'],
                        'confidence': adaptive_prediction['confidence']
                    })
            except Exception as e:
                logger.error(f"{coin} ì ì‘í˜• ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

        if not predictions:
            return None

        # 3. ì•™ìƒë¸” ì˜ˆì¸¡ (ê°€ì¤‘ í‰ê· )
        total_weight = sum(p['confidence'] for p in predictions)
        if total_weight == 0:
            return None

        ensemble_prediction = sum(p['predicted_change'] * p['confidence'] for p in predictions) / total_weight
        ensemble_confidence = min(total_weight / len(predictions), 1.0)

        # í˜„ì¬ ê°€ê²© ì •ë³´
        ticker = self.upbit.GetTicker([f'KRW-{coin}'])
        if not ticker:
            return None

        current_price = ticker[0].trade_price
        predicted_price = current_price * (1 + ensemble_prediction / 100)

        prediction_result = {
            'coin': coin,
            'current_price': current_price,
            'predicted_change_pct': ensemble_prediction,
            'predicted_price': predicted_price,
            'horizon_hours': horizon_hours,
            'timestamp': datetime.now(),
            'confidence': ensemble_confidence,
            'ensemble_details': predictions,
            'adaptive_learning': self.adaptive_learning_enabled
        }

        # ì ì‘í˜• í•™ìŠµì— ì˜ˆì¸¡ ê¸°ë¡ (ë‚˜ì¤‘ì— ê²€ì¦ìš©)
        if self.adaptive_learning_enabled:
            self.record_prediction_for_later_verification(coin, ensemble_prediction, ensemble_confidence)

        return prediction_result

    def record_prediction_for_later_verification(self, coin, predicted_change, confidence):
        """ì˜ˆì¸¡ì„ ë‚˜ì¤‘ì— ê²€ì¦í•˜ê¸° ìœ„í•´ ê¸°ë¡"""
        if not hasattr(self, 'pending_predictions'):
            self.pending_predictions = deque(maxlen=1000)

        self.pending_predictions.append({
            'coin': coin,
            'predicted_change': predicted_change,
            'confidence': confidence,
            'prediction_time': datetime.now(),
            'current_price': self.upbit.GetTicker([f'KRW-{coin}'])[0].trade_price if self.upbit.GetTicker([f'KRW-{coin}']) else 0
        })

    def verify_and_learn_from_predictions(self):
        """ì˜ˆì¸¡ ê²€ì¦ ë° í•™ìŠµ"""
        if not hasattr(self, 'pending_predictions') or not self.adaptive_learning_enabled:
            return

        verified_count = 0
        for prediction in list(self.pending_predictions):
            # 30ë¶„ í›„ ê²€ì¦
            time_elapsed = (datetime.now() - prediction['prediction_time']).total_seconds()
            if time_elapsed >= 1800:  # 30ë¶„
                coin = prediction['coin']
                try:
                    current_ticker = self.upbit.GetTicker([f'KRW-{coin}'])
                    if current_ticker:
                        current_price = current_ticker[0].trade_price
                        actual_change = (current_price - prediction['current_price']) / prediction['current_price'] * 100

                        # ì ì‘í˜• í•™ìŠµì— í”¼ë“œë°±
                        self.adaptive_learner.learn_from_prediction_feedback(
                            coin,
                            prediction['predicted_change'],
                            actual_change,
                            prediction['confidence']
                        )

                        verified_count += 1

                    # ê²€ì¦ëœ ì˜ˆì¸¡ ì œê±°
                    self.pending_predictions.remove(prediction)

                except Exception as e:
                    logger.error(f"ì˜ˆì¸¡ ê²€ì¦ ì‹¤íŒ¨ ({coin}): {e}")

        if verified_count > 0:
            logger.info(f"ğŸ¯ {verified_count}ê°œ ì˜ˆì¸¡ ê²€ì¦ ë° í•™ìŠµ ì™„ë£Œ")

    def execute_trade_with_learning(self, coin, signal, price, quantity=None):
        """í•™ìŠµ ê¸°ëŠ¥ì´ í¬í•¨ëœ ê±°ë˜ ì‹¤í–‰"""
        try:
            # ê±°ë˜ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§)
            trade_executed = False
            if signal == 'BUY':
                logger.info(f"ğŸ’° ë§¤ìˆ˜ ì‹ í˜¸ ì‹¤í–‰: {coin} @ {price:,}ì›")
                trade_executed = True
            elif signal == 'SELL':
                logger.info(f"ğŸ’° ë§¤ë„ ì‹ í˜¸ ì‹¤í–‰: {coin} @ {price:,}ì›")
                trade_executed = True

            # ì ì‘í˜• í•™ìŠµì— ê±°ë˜ ê¸°ë¡
            if trade_executed and self.adaptive_learning_enabled:
                self.adaptive_learner.experience_collector.record_trade(
                    coin=coin,
                    action=signal,
                    price=price,
                    quantity=quantity or 0
                )

            return trade_executed

        except Exception as e:
            logger.error(f"ê±°ë˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False

    def continuous_learning_update(self):
        """ì§€ì†ì  í•™ìŠµ ì—…ë°ì´íŠ¸"""
        if not self.adaptive_learning_enabled:
            return

        try:
            # ì˜ˆì¸¡ ê²€ì¦
            self.verify_and_learn_from_predictions()

            # ì§€ì†ì  í•™ìŠµ ì‚¬ì´í´
            self.adaptive_learner.continuous_learning_cycle()

            # í•™ìŠµ ìƒíƒœ ì €ì¥
            self.adaptive_learner.save_learning_state()

            logger.info("ğŸ”„ ì§€ì†ì  í•™ìŠµ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ì§€ì†ì  í•™ìŠµ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def generate_trading_signals(self):
        """ê±°ë˜ ì‹ í˜¸ ìƒì„±"""
        signals = {}

        for coin in ['BTC', 'ETH', 'ADA', 'DOT']:
            prediction = self.predict_price_movement(coin)
            if not prediction:
                signals[coin] = 'HOLD'
                continue

            predicted_change = prediction['predicted_change_pct']
            confidence = prediction['confidence']

            # ì‹ í˜¸ ìƒì„± ë¡œì§ (ë³´ìˆ˜ì )
            if predicted_change > 2.0 and confidence > 0.6:
                signals[coin] = 'BUY'
            elif predicted_change < -2.0 and confidence > 0.6:
                signals[coin] = 'SELL'
            else:
                signals[coin] = 'HOLD'

            logger.info(f"ğŸ¯ {coin}: {signals[coin]} (ì˜ˆì¸¡: {predicted_change:+.2f}%, ì‹ ë¢°ë„: {confidence:.2f})")

        return signals

    def save_models(self, filepath='models/news_sentiment_models.pkl'):
        """ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)

            model_data = {
                'models': self.prediction_models,
                'scalers': self.scalers,
                'news_history': list(self.news_history)[-100:],  # ìµœê·¼ 100ê°œë§Œ
                'timestamp': datetime.now()
            }

            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)

            logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")

        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_models(self, filepath='models/news_sentiment_models.pkl'):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'rb') as f:
                    model_data = pickle.load(f)

                self.prediction_models = model_data['models']
                self.scalers = model_data['scalers']

                if 'news_history' in model_data:
                    self.news_history.extend(model_data['news_history'])

                logger.info(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
                return True
            else:
                logger.info("ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.")
                return False

        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def run_one_hour_learning(self):
        """1ì‹œê°„ ì œí•œ í•™ìŠµ"""
        learning_start = time.time()
        logger.info("ğŸ”¥ 1ì‹œê°„ ì§‘ì¤‘ í•™ìŠµ ì‹œì‘!")

        try:
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
            self.load_models()

            # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ (ìµœëŒ€ 20ë¶„)
            logger.info("ğŸ“° 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°ì • ë¶„ì„")
            for cycle in range(3):  # 3ë²ˆ ìˆ˜ì§‘
                elapsed = time.time() - learning_start
                if elapsed > self.max_learning_time * 0.4:  # 40% ì‹œê°„ ì´ˆê³¼ì‹œ ì¤‘ë‹¨
                    logger.warning("â° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œê°„ ì´ˆê³¼, ë‹¤ìŒ ë‹¨ê³„ë¡œ")
                    break

                logger.info(f"ğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘ ì‚¬ì´í´ {cycle + 1}/3")
                coin_sentiments = self.collect_and_analyze_news()

                if cycle < 2:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ì ê¹ ëŒ€ê¸°
                    time.sleep(30)

            # 2. ëª¨ë¸ í›ˆë ¨ (ìµœëŒ€ 30ë¶„)
            elapsed = time.time() - learning_start
            remaining_time = self.max_learning_time - elapsed

            if remaining_time > 600:  # 10ë¶„ ì´ìƒ ë‚¨ì•˜ì„ ë•Œë§Œ
                logger.info("ğŸ“ 2ë‹¨ê³„: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨")
                self.train_quick_models()
            else:
                logger.warning("â° ëª¨ë¸ í›ˆë ¨ ì‹œê°„ ë¶€ì¡±, ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©")

            # 3. ëª¨ë¸ ì €ì¥
            self.save_models()

            # 4. ì´ˆê¸° ì˜ˆì¸¡ ìƒì„±
            logger.info("ğŸ”® 3ë‹¨ê³„: ì´ˆê¸° ì˜ˆì¸¡ ìƒì„±")
            signals = self.generate_trading_signals()

            total_elapsed = time.time() - learning_start
            logger.info(f"âœ… 1ì‹œê°„ í•™ìŠµ ì™„ë£Œ! ({total_elapsed/60:.1f}ë¶„ ì†Œìš”)")
            logger.info("ğŸš€ íŠ¸ë ˆì´ë”© ì¤€ë¹„ ì™„ë£Œ!")

            return True

        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def run_trading_cycle(self):
        """1ì‹œê°„ íŠ¸ë ˆì´ë”© ì‚¬ì´í´"""
        logger.info("ğŸ’° 1ì‹œê°„ íŠ¸ë ˆì´ë”© ì‹œì‘!")

        trading_start = time.time()
        cycle_count = 0

        try:
            while time.time() - trading_start < 3600:  # 1ì‹œê°„
                cycle_count += 1
                logger.info(f"ğŸ”„ íŠ¸ë ˆì´ë”© ì‚¬ì´í´ #{cycle_count}")

                # 1. ìƒˆë¡œìš´ ë‰´ìŠ¤ ìˆ˜ì§‘ (ê°„ì†Œí™”)
                if cycle_count % 3 == 1:  # 3ì‚¬ì´í´ë§ˆë‹¤
                    coin_sentiments = self.collect_and_analyze_news()

                # 2. ê±°ë˜ ì‹ í˜¸ ìƒì„±
                signals = self.generate_trading_signals()

                # 3. ê±°ë˜ ì‹¤í–‰ (í•™ìŠµ í¬í•¨)
                for coin, signal in signals.items():
                    if signal != 'HOLD':
                        ticker = self.upbit.GetTicker([f'KRW-{coin}'])
                        if ticker:
                            current_price = ticker[0].trade_price
                            self.execute_trade_with_learning(coin, signal, current_price)

                # 4. ì§€ì†ì  í•™ìŠµ ì—…ë°ì´íŠ¸ (5ë¶„ë§ˆë‹¤)
                if cycle_count % 3 == 0:  # 15ë¶„ë§ˆë‹¤ (3ì‚¬ì´í´ * 5ë¶„)
                    self.continuous_learning_update()

                # 10ë¶„ ëŒ€ê¸°
                logger.info("â±ï¸ 10ë¶„ ëŒ€ê¸°...")
                time.sleep(600)

        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ì‚¬ìš©ìì— ì˜í•œ íŠ¸ë ˆì´ë”© ì¤‘ì§€")
        except Exception as e:
            logger.error(f"âŒ íŠ¸ë ˆì´ë”© ì¤‘ ì˜¤ë¥˜: {e}")

        total_elapsed = time.time() - trading_start
        logger.info(f"ğŸ íŠ¸ë ˆì´ë”© ì™„ë£Œ ({total_elapsed/60:.1f}ë¶„ ì†Œìš”)")

def main():
    """ë©”ì¸ í•¨ìˆ˜ - 1ì‹œê°„ í•™ìŠµ + 1ì‹œê°„ íŠ¸ë ˆì´ë”©"""
    print("ğŸŒ ë‰´ìŠ¤ ê¸°ë°˜ ì½”ì¸ ì˜ˆì¸¡ AI")
    print("=" * 50)

    try:
        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ì‹œ)
        try:
            import nltk
            nltk.download('vader_lexicon', quiet=True)
            nltk.download('punkt', quiet=True)
        except:
            pass

        ai = NewsSentimentAI()

        while True:
            logger.info("ğŸš€ ìƒˆë¡œìš´ í•™ìŠµ+íŠ¸ë ˆì´ë”© ì£¼ê¸° ì‹œì‘!")

            # 1ë‹¨ê³„: 1ì‹œê°„ ì§‘ì¤‘ í•™ìŠµ
            logger.info("=" * 60)
            logger.info("ğŸ“š PHASE 1: 1ì‹œê°„ ì§‘ì¤‘ í•™ìŠµ")
            logger.info("=" * 60)

            success = ai.run_one_hour_learning()

            if success:
                # 2ë‹¨ê³„: 1ì‹œê°„ íŠ¸ë ˆì´ë”©
                logger.info("=" * 60)
                logger.info("ğŸ’° PHASE 2: 1ì‹œê°„ íŠ¸ë ˆì´ë”©")
                logger.info("=" * 60)

                ai.run_trading_cycle()
            else:
                logger.error("âŒ í•™ìŠµ ì‹¤íŒ¨ë¡œ ì´ë²ˆ ì£¼ê¸° ê±´ë„ˆëœ€")
                time.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸°

            logger.info("ğŸ”„ ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ì ì‹œ ëŒ€ê¸°...")
            time.sleep(300)  # 5ë¶„ íœ´ì‹

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ì‚¬ìš©ìì— ì˜í•œ ì‹œìŠ¤í…œ ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()